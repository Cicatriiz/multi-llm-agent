#!/usr/bin/env python3
"""
Comprehensive test suite for AB-MCTS implementation with performance analysis.
"""

import sys
import os
import random
import time
import logging
from typing import Dict, Any, Optional, Tuple, List
import statistics

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class SmartAgent:
    """More sophisticated mock agent with different specializations."""
    
    def __init__(self, name: str, specialization: str = "general", quality: float = 0.8):
        self.name = name
        self.specialization = specialization
        self.quality = quality
        self.call_count = 0
        self.generation_times = []
    
    def generate(self, prompt: str) -> Dict[str, Any]:
        """Generate a response based on specialization."""
        start_time = time.time()
        self.call_count += 1
        
        # Different response patterns based on specialization
        if self.specialization == "reasoning":
            responses = [
                f"Let me analyze '{prompt}' step by step. First, we need to consider...",
                f"The logical approach to '{prompt}' involves breaking it down into components...",
                f"From a reasoning perspective, '{prompt}' requires systematic thinking...",
            ]
            quality_bonus = 0.15
        elif self.specialization == "creative":
            responses = [
                f"Here's an innovative approach to '{prompt}' that thinks outside the box...",
                f"Let me propose a creative solution for '{prompt}' that combines unique elements...",
                f"Taking a fresh perspective on '{prompt}', I suggest...",
            ]
            quality_bonus = 0.1
        elif self.specialization == "technical":
            responses = [
                f"The technical implementation for '{prompt}' should focus on scalability...",
                f"From an engineering standpoint, '{prompt}' requires robust architecture...",
                f"The optimal technical solution for '{prompt}' involves...",
            ]
            quality_bonus = 0.12
        else:  # general
            responses = [
                f"Regarding '{prompt}', I recommend a balanced approach that considers...",
                f"For '{prompt}', we should evaluate multiple factors including...",
                f"The best way to handle '{prompt}' is through careful analysis...",
            ]
            quality_bonus = 0.0
        
        content = random.choice(responses) + f" [Generated by {self.name}]"
        
        # Simulate thinking time based on specialization
        think_time = random.uniform(0.001, 0.005)  # 1-5ms
        time.sleep(think_time)
        
        # Quality varies based on agent and prompt match
        effective_quality = min(1.0, self.quality + quality_bonus)
        
        if random.random() < effective_quality:
            content += " This response includes detailed analysis and practical recommendations."
        
        generation_time = time.time() - start_time
        self.generation_times.append(generation_time)
        
        return {
            "content": content,
            "model": self.name,
            "specialization": self.specialization,
            "usage": {"tokens": len(content.split())},
            "quality": effective_quality,
            "generation_time": generation_time
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """Get performance statistics for this agent."""
        if not self.generation_times:
            return {"calls": 0, "avg_time": 0, "total_time": 0}
        
        return {
            "calls": self.call_count,
            "avg_time": statistics.mean(self.generation_times),
            "total_time": sum(self.generation_times),
            "min_time": min(self.generation_times),
            "max_time": max(self.generation_times)
        }

def create_diverse_agents() -> Dict[str, SmartAgent]:
    """Create a diverse set of agents with different capabilities."""
    return {
        "reasoning_expert": SmartAgent("reasoning_expert", "reasoning", quality=0.9),
        "creative_writer": SmartAgent("creative_writer", "creative", quality=0.85),
        "tech_specialist": SmartAgent("tech_specialist", "technical", quality=0.88),
        "general_assistant": SmartAgent("general_assistant", "general", quality=0.8),
        "quick_responder": SmartAgent("quick_responder", "general", quality=0.75),
    }

def test_algorithm_performance(algorithm_name: str, num_iterations: int = 20) -> Dict[str, Any]:
    """Test a specific algorithm's performance."""
    
    try:
        if algorithm_name == "ab_mcts_a":
            from multi_llm_agent.ab_mcts import ABMCTSA as Algorithm
        else:
            from multi_llm_agent.ab_mcts import ABMCTSM as Algorithm
        
        from multi_llm_agent.ab_mcts import top_k
        from multi_llm_agent.ab_mcts.algorithm import LLMResponse
        
        agents = create_diverse_agents()
        
        # Create generation functions
        def create_generate_fn(agent, name):
            def generate_fn(parent_state=None):
                prompt = "How can we improve machine learning model performance?"
                if parent_state:
                    prompt = f"Building on previous insight: '{parent_state.content[:50]}...', how can we further improve?"
                
                result = agent.generate(prompt)
                
                # More sophisticated scoring based on content and agent quality
                base_score = result["quality"]
                length_bonus = min(0.2, len(result["content"]) / 1000)  # Bonus for detailed responses
                specialization_bonus = 0.1 if agent.specialization in prompt.lower() else 0
                
                final_score = min(1.0, base_score + length_bonus + specialization_bonus)
                
                response = LLMResponse(
                    content=result["content"],
                    model_name=name,
                    score=final_score,
                    metadata=result
                )
                return response, response.score
            return generate_fn
        
        generate_fns = {name: create_generate_fn(agent, name) for name, agent in agents.items()}
        
        # Run the algorithm
        start_time = time.time()
        algo = Algorithm()
        state = algo.init_tree()
        
        iteration_scores = []
        tree_sizes = []
        
        for i in range(num_iterations):
            iter_start = time.time()
            state = algo.step(state, generate_fns)
            iter_time = time.time() - iter_start
            
            # Track progress
            if len(state.tree.get_state_score_pairs()) > 0:
                current_best = max(state.tree.get_state_score_pairs(), key=lambda x: x[1])[1]
                iteration_scores.append(current_best)
            
            tree_sizes.append(len(state.tree))
            
            if (i + 1) % 5 == 0:
                logger.info(f"  {algorithm_name.upper()} iteration {i+1}: tree size = {len(state.tree)}, iteration time = {iter_time:.4f}s")
        
        total_time = time.time() - start_time
        
        # Get final results
        results = top_k(state, algo, k=5)
        
        # Analyze tree structure
        nodes = state.tree.get_nodes()
        depths = [node.depth for node in nodes if not node.is_root()]
        
        # Agent usage statistics
        agent_stats = {name: agent.get_stats() for name, agent in agents.items()}
        
        return {
            "algorithm": algorithm_name,
            "total_time": total_time,
            "iterations": num_iterations,
            "final_tree_size": len(state.tree),
            "best_score": results[0][1] if results else 0.0,
            "avg_score": statistics.mean([r[1] for r in results]) if results else 0.0,
            "score_progression": iteration_scores,
            "tree_growth": tree_sizes,
            "max_depth": max(depths) if depths else 0,
            "avg_depth": statistics.mean(depths) if depths else 0,
            "agent_stats": agent_stats,
            "results": results[:3],  # Top 3 results
        }
        
    except Exception as e:
        logger.error(f"Error testing {algorithm_name}: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}

def compare_algorithms():
    """Compare AB-MCTS-A and AB-MCTS-M performance."""
    
    logger.info("ðŸ”¬ Comparing AB-MCTS Algorithms")
    logger.info("=" * 60)
    
    algorithms = ["ab_mcts_a", "ab_mcts_m"]
    results = {}
    
    for algo in algorithms:
        logger.info(f"\nðŸ§ª Testing {algo.upper()}...")
        results[algo] = test_algorithm_performance(algo, num_iterations=25)
    
    # Analysis
    logger.info("\nðŸ“Š PERFORMANCE COMPARISON")
    logger.info("=" * 60)
    
    for algo, result in results.items():
        if "error" in result:
            logger.error(f"{algo.upper()}: ERROR - {result['error']}")
            continue
            
        logger.info(f"\n{algo.upper()} Results:")
        logger.info(f"  â€¢ Best Score: {result['best_score']:.4f}")
        logger.info(f"  â€¢ Average Score: {result['avg_score']:.4f}")
        logger.info(f"  â€¢ Total Time: {result['total_time']:.3f}s")
        logger.info(f"  â€¢ Final Tree Size: {result['final_tree_size']} nodes")
        logger.info(f"  â€¢ Max Depth: {result['max_depth']}")
        logger.info(f"  â€¢ Average Depth: {result['avg_depth']:.2f}")
        
        # Agent usage
        total_calls = sum(stats['calls'] for stats in result['agent_stats'].values())
        logger.info(f"  â€¢ Total Agent Calls: {total_calls}")
        
        most_used = max(result['agent_stats'].items(), key=lambda x: x[1]['calls'])
        logger.info(f"  â€¢ Most Used Agent: {most_used[0]} ({most_used[1]['calls']} calls)")
        
        # Top results
        logger.info(f"  â€¢ Top 3 Results:")
        for i, (response, score) in enumerate(result['results'], 1):
            logger.info(f"    {i}. {response.model_name}: {score:.4f}")
    
    # Comparison summary
    if len(results) == 2 and all("error" not in r for r in results.values()):
        a_result = results["ab_mcts_a"]
        m_result = results["ab_mcts_m"]
        
        logger.info(f"\nðŸ† WINNER ANALYSIS:")
        logger.info(f"  â€¢ Best Score: {'AB-MCTS-A' if a_result['best_score'] > m_result['best_score'] else 'AB-MCTS-M'}")
        logger.info(f"  â€¢ Faster: {'AB-MCTS-A' if a_result['total_time'] < m_result['total_time'] else 'AB-MCTS-M'}")
        logger.info(f"  â€¢ More Exploratory: {'AB-MCTS-A' if a_result['final_tree_size'] > m_result['final_tree_size'] else 'AB-MCTS-M'}")
    
    return results

def test_scaling_behavior():
    """Test how algorithms scale with different search budgets."""
    
    logger.info("\nðŸ”¬ Testing Scaling Behavior")
    logger.info("=" * 60)
    
    budgets = [5, 10, 20, 30]
    scaling_results = {}
    
    for budget in budgets:
        logger.info(f"\nTesting with search budget: {budget}")
        scaling_results[budget] = {}
        
        for algo in ["ab_mcts_a", "ab_mcts_m"]:
            result = test_algorithm_performance(algo, num_iterations=budget)
            if "error" not in result:
                scaling_results[budget][algo] = {
                    "best_score": result["best_score"],
                    "time": result["total_time"],
                    "tree_size": result["final_tree_size"]
                }
    
    # Analysis
    logger.info("\nðŸ“ˆ SCALING ANALYSIS:")
    for algo in ["ab_mcts_a", "ab_mcts_m"]:
        logger.info(f"\n{algo.upper()} Scaling:")
        logger.info("Budget\tBest Score\tTime(s)\tTree Size")
        for budget in budgets:
            if budget in scaling_results and algo in scaling_results[budget]:
                data = scaling_results[budget][algo]
                logger.info(f"{budget}\t{data['best_score']:.4f}\t\t{data['time']:.3f}\t{data['tree_size']}")
    
    return scaling_results

def main():
    """Run comprehensive tests."""
    
    logger.info("ðŸ§ª AB-MCTS Comprehensive Test Suite")
    logger.info("=" * 70)
    
    try:
        # Basic functionality test
        logger.info("Running basic functionality test...")
        os.system("python test_ab_mcts_simple.py")
        
        # Performance comparison
        comparison_results = compare_algorithms()
        
        # Scaling behavior
        scaling_results = test_scaling_behavior()
        
        logger.info("\nâœ… All comprehensive tests completed!")
        return 0
        
    except Exception as e:
        logger.error(f"Comprehensive test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
